#!/usr/bin/env python3
"""
Revision Quality Test - Testet die QualitÃ¤t des Revisionsprompts
Vergleicht verschiedene Revision-Szenarien und misst ob Ã„nderungen gezielt sind.
"""

import json
import os
import sys
from datetime import datetime
from dataclasses import dataclass, asdict
from typing import List, Optional

# Projektroot zum Path hinzufÃ¼gen
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config import OPENAI_API_KEY

# Mock-Artikel fÃ¼r Tests (realistischer kurzer Artikel)
MOCK_ARTICLE = """# EinfÃ¼hrung in Cloud Computing fÃ¼r die Ã¶ffentliche Verwaltung

## Executive Summary

Cloud Computing bietet der Ã¶ffentlichen Verwaltung erhebliche Vorteile in Bezug auf Kosteneffizienz, Skalierbarkeit und Modernisierung der IT-Infrastruktur [1]. Diese Ãœbersicht analysiert die wichtigsten Aspekte und Herausforderungen.

## 1. Grundlagen des Cloud Computing

Cloud Computing bezeichnet die Bereitstellung von IT-Ressourcen Ã¼ber das Internet [2]. Die drei Hauptmodelle sind Infrastructure as a Service (IaaS), Platform as a Service (PaaS) und Software as a Service (SaaS) [3].

### 1.1 Vorteile fÃ¼r BehÃ¶rden

Die Vorteile umfassen reduzierte Infrastrukturkosten, erhÃ¶hte FlexibilitÃ¤t und verbesserte Zusammenarbeit zwischen Abteilungen [4]. Besonders relevant ist die MÃ¶glichkeit zur schnellen Skalierung bei Bedarfsspitzen.

## 2. Sicherheitsaspekte

Datenschutz und Compliance sind zentrale Herausforderungen [5]. Die DSGVO stellt besondere Anforderungen an die Verarbeitung personenbezogener Daten in der Cloud.

## 3. Fazit

Cloud Computing ist ein wichtiger Baustein der Verwaltungsdigitalisierung, erfordert aber sorgfÃ¤ltige Planung und Umsetzung.

## Quellenverzeichnis

[1] Bundesministerium des Innern: Cloud-Strategie der Bundesverwaltung, 2024
[2] Bitkom: Cloud Computing Leitfaden, 2023
[3] NIST: Definition of Cloud Computing, 2011
[4] Gartner: Government Cloud Adoption Report, 2024
[5] BSI: Cloud Computing Grundlagen, 2023
"""

@dataclass
class TestIssue:
    type: str
    severity: str
    description: str
    suggested_action: str

@dataclass
class TestVerdict:
    verdict: str
    confidence: float
    summary: str
    issues: List[TestIssue]

@dataclass
class RevisionResult:
    scenario: str
    original_words: int
    revised_words: int
    word_change_percent: float
    tokens_used: int
    cost_usd: float
    revision_focused: bool  # Subjektive Bewertung ob gezielt
    notes: str

def create_test_scenarios() -> List[tuple]:
    """Erstellt verschiedene Test-Szenarien mit unterschiedlichen Issues."""
    
    scenarios = [
        # Szenario 1: Nur Quellenprobleme
        (
            "sources_only",
            TestVerdict(
                verdict="revise",
                confidence=0.7,
                summary="Der Artikel hat gute Struktur, aber einige Aussagen sind nicht belegt.",
                issues=[
                    TestIssue(
                        type="sources",
                        severity="medium",
                        description="In Abschnitt 1.1 fehlen Quellenverweise fÃ¼r die genannten Vorteile",
                        suggested_action="FÃ¼ge Quellenverweise [X] fÃ¼r die Aussagen zu Kosteneinsparungen hinzu"
                    )
                ]
            )
        ),
        
        # Szenario 2: Strukturproblem
        (
            "structure_only",
            TestVerdict(
                verdict="revise",
                confidence=0.75,
                summary="Dem Artikel fehlt ein wichtiger Abschnitt.",
                issues=[
                    TestIssue(
                        type="structure",
                        severity="high",
                        description="Es fehlt ein Abschnitt zu 'Limitationen und Risiken'",
                        suggested_action="ErgÃ¤nze einen Abschnitt 2.1 'Risiken und Limitationen' nach den Sicherheitsaspekten"
                    )
                ]
            )
        ),
        
        # Szenario 3: InhaltslÃ¼cke
        (
            "content_gap",
            TestVerdict(
                verdict="revise",
                confidence=0.65,
                summary="Ein wichtiges Thema wird nicht ausreichend behandelt.",
                issues=[
                    TestIssue(
                        type="content_gap",
                        severity="medium",
                        description="Das Thema 'Sovereign Cloud' und deutsche Cloud-Anbieter fehlt komplett",
                        suggested_action="Erweitere Abschnitt 2 um einen Unterabschnitt zu deutschen/europÃ¤ischen Cloud-Anbietern"
                    )
                ]
            )
        ),
        
        # Szenario 4: Multiple Issues (realistisch)
        (
            "multiple_issues",
            TestVerdict(
                verdict="revise",
                confidence=0.6,
                summary="Mehrere kleinere Verbesserungen nÃ¶tig.",
                issues=[
                    TestIssue(
                        type="sources",
                        severity="low",
                        description="Aussage zu 'schneller Skalierung' ohne Beleg",
                        suggested_action="Quellenangabe ergÃ¤nzen"
                    ),
                    TestIssue(
                        type="consistency",
                        severity="medium",
                        description="Executive Summary erwÃ¤hnt 'Kosteneffizienz', aber der Haupttext geht nicht darauf ein",
                        suggested_action="Im Haupttext konkrete Zahlen oder Beispiele zu Kosteneffizienz ergÃ¤nzen"
                    )
                ]
            )
        ),
        
        # Szenario 5: LÃ¤nge explizit genannt
        (
            "length_issue",
            TestVerdict(
                verdict="revise",
                confidence=0.7,
                summary="Der Abschnitt zu Sicherheit ist zu oberflÃ¤chlich.",
                issues=[
                    TestIssue(
                        type="length",
                        severity="medium",
                        description="Abschnitt 2 (Sicherheitsaspekte) ist mit nur 2 SÃ¤tzen zu kurz",
                        suggested_action="Erweitere den Sicherheitsabschnitt um konkrete MaÃŸnahmen und BSI-Empfehlungen"
                    )
                ]
            )
        )
    ]
    
    return scenarios


def run_revision_test(scenario_name: str, verdict: TestVerdict, article: str) -> RevisionResult:
    """FÃ¼hrt einen einzelnen Revisionstest durch."""
    
    from openai import OpenAI
    
    # Issues formatieren
    issues_text = ""
    for issue in verdict.issues:
        issues_text += f"- [{issue.severity.upper()}] {issue.type}: {issue.description}\n"
        issues_text += f"  Aktion: {issue.suggested_action}\n"
    
    current_word_count = len(article.split())
    
    # Der NEUE qualitÃ¤tsfokussierte Prompt
    prompt = f"""Du bist ein erfahrener wissenschaftlicher Lektor. Deine Aufgabe ist eine GEZIELTE ÃœBERARBEITUNG.

# EDITOR-FEEDBACK
{verdict.summary}

## Zu behebende Probleme:
{issues_text}

# AKTUELLER ARTIKEL
{article}

# ÃœBERARBEITUNGSANLEITUNG

## Dein Auftrag
Behebe EXAKT die oben genannten Probleme. Nicht mehr, nicht weniger.

## Issue-spezifische MaÃŸnahmen
- "sources": FÃ¼ge an den kritisierten Stellen fehlende Quellenverweise [X] ein
- "structure": ErgÃ¤nze konkret die fehlenden Abschnitte (z.B. Executive Summary, Limitations)
- "content_gap": Vertiefe GENAU die genannten Themen mit den neuen Quellen
- "consistency": Korrigiere PRÃ„ZISE die genannten WidersprÃ¼che
- "length": Erweitere die KONKRET kritisierten dÃ¼nnen Passagen

## QualitÃ¤tsprinzipien
1. CHIRURGISCHE PRÃ„ZISION: Ã„ndere nur, was kritisiert wurde
2. KONTEXT BEWAHREN: Bestehende gute Passagen bleiben unverÃ¤ndert
3. QUELLENINTEGRITÃ„T: Alle [X]-Verweise mÃ¼ssen erhalten bleiben
4. VOLLSTÃ„NDIGKEIT: Gib den GESAMTEN Artikel zurÃ¼ck (nicht nur Ã„nderungen)

## WICHTIG
- Keine proaktiven "Verbesserungen" an Stellen ohne Kritik
- Kein FÃ¼lltext - jede ErgÃ¤nzung muss einen Issue adressieren
- Der wissenschaftliche Ton bleibt durchgehend sachlich

ÃœBERARBEITETER ARTIKEL:"""

    # API-Aufruf
    client = OpenAI(api_key=OPENAI_API_KEY)
    
    response = client.chat.completions.create(
        model="gpt-4o-mini",  # GÃ¼nstigeres Modell fÃ¼r Tests
        messages=[{"role": "user", "content": prompt}],
        max_completion_tokens=4000
    )
    
    revised_article = response.choices[0].message.content
    revised_words = len(revised_article.split()) if revised_article else 0
    
    tokens_input = response.usage.prompt_tokens
    tokens_output = response.usage.completion_tokens
    total_tokens = tokens_input + tokens_output
    
    # Kosten fÃ¼r gpt-4o-mini: $0.15/1M input, $0.60/1M output
    cost = (tokens_input * 0.00015 / 1000) + (tokens_output * 0.0006 / 1000)
    
    word_change = ((revised_words - current_word_count) / current_word_count) * 100
    
    # Einfache Heuristik fÃ¼r "fokussierte Revision"
    # Wenn Ã„nderung < 30% und nicht negativ bei non-length issues
    if "length" in [i.type for i in verdict.issues]:
        focused = revised_words > current_word_count  # Sollte lÃ¤nger sein
    else:
        focused = abs(word_change) < 30  # Sollte nicht zu viel Ã¤ndern
    
    return RevisionResult(
        scenario=scenario_name,
        original_words=current_word_count,
        revised_words=revised_words,
        word_change_percent=round(word_change, 1),
        tokens_used=total_tokens,
        cost_usd=round(cost, 4),
        revision_focused=focused,
        notes=f"Issues: {[i.type for i in verdict.issues]}"
    )


def run_all_tests():
    """FÃ¼hrt alle Revisionstests durch und speichert Ergebnisse."""
    
    print("=" * 60)
    print("REVISION QUALITY TEST")
    print("=" * 60)
    print(f"Testzeitpunkt: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"Artikel-LÃ¤nge: {len(MOCK_ARTICLE.split())} WÃ¶rter")
    print("=" * 60)
    
    scenarios = create_test_scenarios()
    results = []
    
    for scenario_name, verdict in scenarios:
        print(f"\nðŸ§ª Test: {scenario_name}")
        print(f"   Issues: {[i.type for i in verdict.issues]}")
        
        try:
            result = run_revision_test(scenario_name, verdict, MOCK_ARTICLE)
            results.append(result)
            
            status = "âœ…" if result.revision_focused else "âš ï¸"
            print(f"   {status} WÃ¶rter: {result.original_words} â†’ {result.revised_words} ({result.word_change_percent:+.1f}%)")
            print(f"   ðŸ’° Kosten: ${result.cost_usd:.4f} ({result.tokens_used} tokens)")
            
        except Exception as e:
            print(f"   âŒ Fehler: {e}")
            results.append(RevisionResult(
                scenario=scenario_name,
                original_words=len(MOCK_ARTICLE.split()),
                revised_words=0,
                word_change_percent=0,
                tokens_used=0,
                cost_usd=0,
                revision_focused=False,
                notes=f"ERROR: {str(e)}"
            ))
    
    # Zusammenfassung
    print("\n" + "=" * 60)
    print("ZUSAMMENFASSUNG")
    print("=" * 60)
    
    total_cost = sum(r.cost_usd for r in results)
    focused_count = sum(1 for r in results if r.revision_focused)
    
    print(f"Tests durchgefÃ¼hrt: {len(results)}")
    print(f"Fokussierte Revisionen: {focused_count}/{len(results)}")
    print(f"Gesamtkosten: ${total_cost:.4f}")
    
    # Detailtabelle
    print("\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("â”‚ Szenario            â”‚ Vorher â”‚ Nachherâ”‚ Ã„nderung â”‚ Fokus?  â”‚")
    print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    for r in results:
        status = "âœ…" if r.revision_focused else "âŒ"
        print(f"â”‚ {r.scenario:<19} â”‚ {r.original_words:>6} â”‚ {r.revised_words:>6} â”‚ {r.word_change_percent:>+7.1f}% â”‚ {status:^7} â”‚")
    print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    
    # Ergebnisse speichern
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_file = f"tests/results/revision_quality_{timestamp}.json"
    
    output = {
        "timestamp": datetime.now().isoformat(),
        "prompt_version": "quality_focused_v1",
        "mock_article_words": len(MOCK_ARTICLE.split()),
        "total_cost_usd": total_cost,
        "focused_ratio": f"{focused_count}/{len(results)}",
        "results": [asdict(r) for r in results]
    }
    
    os.makedirs("tests/results", exist_ok=True)
    with open(results_file, "w", encoding="utf-8") as f:
        json.dump(output, f, indent=2, ensure_ascii=False)
    
    print(f"\nðŸ“„ Ergebnisse gespeichert: {results_file}")
    
    return results


if __name__ == "__main__":
    run_all_tests()
