# MAS Design Erfahrungen

Dokumentation unserer Lernreise bei der Entwicklung eines Multi-Agenten-Systems f√ºr hochwertige Wissensartikel.

---

## Design 1: "Search-First" (Recherche zuerst)

### Ansatz
```
Themenanalyse ‚Üí Recherche-Plan ‚Üí X Runden Suche ‚Üí Writer fasst zusammen ‚Üí Editor pr√ºft
```

### Wie es funktionierte
1. Orchestrator analysiert Frage, erstellt Recherche-Plan
2. Researcher f√ºhrt 5-8 Suchrunden mit verschiedenen Tools durch
3. Jede Runde: Keyword-basierte Suche (Tavily, Wikipedia, etc.)
4. Writer bekommt alle Recherche-Ergebnisse, schreibt Artikel
5. Editor pr√ºft optional

### Ergebnis
- ‚úÖ **Viele Quellen** (15-25 externe Referenzen)
- ‚úÖ **Diverse Tools** (Wikipedia, Tavily, GNews, HackerNews, etc.)

### Das Problem
- ‚ùå **LLM "vergisst" eigenes Wissen**
- ‚ùå **Sucht nach falschen Begriffen** 

**Konkretes Beispiel:**  
Frage: *"Was ist ServiceNow mit Agent Builder?"*

- User meinte: **Build Agent** / **AI Agent Studio**
- LLM suchte w√∂rtlich nach: "Agent Builder"
- Tavily fand: Nichts Relevantes
- Ergebnis im Artikel: *"Zu 'Agent Builder' wurden keine Informationen gefunden"*

**Absurdit√§t:** Das LLM WUSSTE was Build Agent ist (aus Training), aber weil es gezwungen war zu suchen, und die Suche nichts fand, schrieb es "nicht gefunden" - obwohl eine direkte ChatGPT-Frage sofort die richtige Antwort geliefert h√§tte.

### Kernproblem
> Das System macht das LLM "dumm", indem es ihm verbietet, sein eigenes Wissen zu nutzen.

---

## Design 2: "Knowledge-First" (LLM-Wissen zuerst)

### Ansatz
```
DraftWriter (GPT) schreibt mit eigenem Wissen 
‚Üí Markiert unsichere Stellen [FACT-CHECK], [RECHERCHE]
‚Üí Gezielte Recherche nur f√ºr Markierungen
‚Üí Integration ‚Üí Editor pr√ºft
```

### Wie es funktionierte
1. DraftWriter (GPT-5.2) erstellt Expertenentwurf aus Training
2. Soll Markierungen setzen: `[FACT-CHECK]`, `[RECHERCHE]`, `[QUELLE]`, `[UNSICHER]`
3. Researcher recherchiert NUR f√ºr markierte Stellen
4. Writer integriert Recherche in Entwurf
5. Editor (Claude) pr√ºft

### Ergebnis
- ‚úÖ **Fundierter Artikel** (18.000+ Zeichen, Expertenwissen)
- ‚úÖ **Build Agent korrekt erkl√§rt** (LLM wusste es!)

### Das Problem
- ‚ùå **0 Markierungen gesetzt** - GPT war zu selbstsicher
- ‚ùå **0 Recherchen durchgef√ºhrt** - weil keine Markierungen
- ‚ùå **Nur 4 Quellen** - alle von ServiceNow selbst
- ‚ùå **Gemini nie verwendet** - obwohl "integriert"
- ‚ùå **Claude sagte "REVISE"** - wurde ignoriert

**Konkretes Ergebnis:**  
- GPT-5.2: 100% der Arbeit
- Claude: Sagte "k√∂nnte besser sein", wurde ignoriert
- Gemini: Gar nicht aufgerufen

### Kernproblem
> Ein Marketing-Artikel mit Extra-Schritten. Wissenschaftlich unbrauchbar (keine unabh√§ngigen Quellen).

**Prof-Test:** *"Das ist ein gut geschriebener Hersteller-Text, keine wissenschaftliche Arbeit."*

---

## Design 3: "Triangulation" (Multi-LLM Kollaboration) - VORSCHLAG

### Kern-Insight
Jedes LLM hat unterschiedliche St√§rken:

| LLM | St√§rke |
|-----|--------|
| **Claude** | Kritisches Denken, gibt Unsicherheiten zu |
| **GPT-5.2** | Breites Wissen, fl√ºssiger Schreibstil |
| **Gemini** | Google Search, Aktualit√§t |

### Ansatz
```
Claude identifiziert L√ºcken ‚Üí Tools + Gemini recherchieren ‚Üí GPT schreibt ‚Üí Claude pr√ºft
```

### Der Flow

#### Phase 1: CLAUDE als kritischer Analyst
Claude bekommt die **Frage** (noch keinen Artikel) und analysiert ehrlich:

```
üß† Was wei√ü ich SICHER?
   ‚Üí ServiceNow ist Enterprise-Plattform, Now Assist existiert...

‚ùì Was ist mir UNSICHER?  
   ‚Üí Ist "Build Agent" offizielles Produkt oder Marketing?
   ‚Üí Wann released? Welche Version?

üìÖ Was braucht AKTUELLE Daten?
   ‚Üí Pricing, neueste Features 2025

üìö Was braucht externe QUELLEN?
   ‚Üí Technische Architektur, Vergleich zu Wettbewerbern
```

**Output:** Konkrete Recherche-Auftr√§ge mit Tool-Zuordnung

#### Phase 2: Gezielte Recherche mit PASSENDEN TOOLS
Nicht "Gemini f√ºr alles", sondern intelligente Tool-Auswahl:

| L√ºcken-Typ | Tool |
|------------|------|
| Wissenschaftliche Frage | Semantic Scholar, arXiv |
| Aktuelle News | GNews |
| Tech-Meinungen | HackerNews |
| Grundlagen | Wikipedia |
| Allgemeine Fakten | Tavily |
| EU/Beh√∂rden | TED |
| Aktualit√§ts-Check | Gemini Search |

#### Phase 3: GPT schreibt den Artikel
GPT bekommt:
- Claudes strukturierte Analyse
- Alle Recherche-Ergebnisse mit Quellen
- Auftrag: Kombiniere eigenes Wissen + externe Fakten

#### Phase 4: Doppelte Pr√ºfung
- **Claude:** Logik, Struktur, alle L√ºcken geschlossen?
- **Gemini:** Aktualit√§t, gibt es neuere Infos?

### Warum das funktionieren sollte

| Problem Design 1 | Problem Design 2 | L√∂sung Design 3 |
|------------------|------------------|-----------------|
| LLM vergisst Wissen | Keine externen Quellen | Claude identifiziert was fehlt |
| Falsche Suchbegriffe | GPT zu selbstsicher | Claude ist ehrlich √ºber Grenzen |
| Zu viel irrelevante Suche | Zu wenig Recherche | Gezielte Recherche f√ºr echte L√ºcken |
| Ein LLM macht alles | Ein LLM macht alles | 3 LLMs mit spezialisierten Rollen |

### Erwartetes Ergebnis
- ‚úÖ Fundiertes Expertenwissen (aus GPT)
- ‚úÖ Ehrliche L√ºcken-Identifikation (durch Claude)
- ‚úÖ Diverse externe Quellen (durch spezialisierte Tools)
- ‚úÖ Aktualit√§t (durch Gemini)
- ‚úÖ Qualit√§tssicherung (durch Claude + Gemini)

---

## Zusammenfassung

| Aspekt | Design 1 | Design 2 | Design 3 |
|--------|----------|----------|----------|
| **Quellen** | Viele (15-25) | Wenige (4) | Gezielt (10-20) |
| **LLM-Wissen** | Ignoriert | √úberdominant | Balanciert |
| **Recherche** | Zu breit | Keine | Gezielt |
| **Multi-LLM** | Nein | Pseudo | Echte Kollaboration |
| **Wissenschaftlich** | Mittelm√§√üig | Mangelhaft | Angestrebt |

---

## Offene Fragen f√ºr Design 3

1. Wie zwingen wir Claude, wirklich ehrlich √ºber Unsicherheiten zu sein?
2. Wie verhindern wir, dass die Tool-Auswahl zu komplex wird?
3. Wie messen wir "Artikelqualit√§t" objektiv?
4. Was ist die optimale Anzahl an Recherchen (Balance: Qualit√§t vs. Kosten)?

---

*Dokumentiert am 19. Januar 2026*
