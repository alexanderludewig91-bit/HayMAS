# HayMAS - Entwicklungs-Dokumentation

Zusammenfassung der Chat-VerlÃ¤ufe mit Cursor AI zur Entwicklung von HayMAS.

---

## ğŸ“… Projekthistorie

### Phase 1: GrundgerÃ¼st (FrÃ¼he Entwicklung)

**Ziel:** Multi-Agenten-System fÃ¼r Wissensartikel-Generierung

**Erstellte Komponenten:**
- `config.py` - API-Keys und Modell-Konfiguration
- `agents/base_agent.py` - Basis-Agent mit ReAct-Loop
- `agents/orchestrator.py` - Workflow-Koordination
- `agents/researcher.py` - Web-Recherche
- `agents/writer.py` - Artikel-Erstellung
- `agents/editor.py` - QualitÃ¤tsprÃ¼fung
- `mcp_server/` - Tool-Server (tavily_search, file_tools)

**LLM-Integration:**
- Anthropic Claude (Opus, Sonnet, Haiku)
- OpenAI GPT (5.2, 5.1, 4o)
- Google Gemini (Deep Research, 3 Pro, 2.5 Flash)

---

### Phase 2: Streamlit UI (Deprecated)

**Datei:** `app.py`

Erste UI-Version mit Streamlit. Probleme:
- `StreamlitAPIException` bei Session-State Updates
- Komplexe State-Verwaltung
- UI sah trotz Iterationen "wie ein Prototyp" aus
- SpÃ¤ter ersetzt durch React-Frontend

**Konzeptionelle Analyse (vor Migration):**
- Streamlit ist gut fÃ¼r Data-Apps, nicht fÃ¼r "Studio"-Feeling
- Problem: Kein klares mentales Modell (Dashboard? Editor? Workflow?)
- LÃ¶sung: Klare Produkt-Metapher "AI Writing Studio" definieren

---

### Phase 3: React Frontend + FastAPI Backend

**Migration von Streamlit zu:**
- `api.py` - FastAPI Backend mit SSE (Server-Sent Events)
- `frontend/` - React + TypeScript + Tailwind CSS

**Workflow:** IDLE â†’ PRODUCING â†’ COMPLETE

---

### Phase 4: Bugfixing Anthropic Tool Calls

**Problem:** `tool_use ids without tool_result blocks` Error

**Ursache:** Claude macht **parallele Tool-Calls**, aber der Code verarbeitete nur den ersten.

**LÃ¶sung:**
1. `_call_claude` sammelt alle `tool_use` Blocks
2. `run()` Loop iteriert durch alle Tool-Calls
3. Tool-Results werden korrekt formatiert

**Relevanter Code in `base_agent.py`:**
```python
# Tool-Result fÃ¼r Anthropic formatieren
self.messages.append({
    "role": "user",
    "content": [{
        "type": "tool_result",
        "tool_use_id": msg["tool_use_id"],
        "content": json.dumps(msg["result"], ensure_ascii=False)
    }]
})
```

---

### Phase 5: Token-Explosion & Rate Limits

**Problem:** Anthropic API `rate_limit_error` (429) - 30.000 Input Tokens/Minute Ã¼berschritten

**Ursache:** Gesamter Konversationsverlauf mit Tool-Results wurde bei jedem API-Call gesendet.

**LÃ¶sung (Option C - Kombiniert):**

1. **Tool-Result Truncation:**
   ```python
   MAX_TOOL_RESULT_CHARS = 1500  # in config.py
   
   def _truncate_result(self, result: Dict) -> Dict:
       # KÃ¼rzt zu lange Ergebnisse automatisch
   ```

2. **Single-Shot Researcher:**
   - Researcher fÃ¼hrt nur EINE Suche pro Aufruf durch
   - Orchestrator ruft Researcher mehrfach auf
   - `researcher.reset()` zwischen den Runden

---

### Phase 6: Infinite Loop Fix

**Problem:** Editor-Writer Feedback-Schleife lief endlos.

**LÃ¶sung:**
```python
MAX_EDITOR_ITERATIONS = 2  # in config.py
```

Orchestrator bricht nach 2 Iterationen ab.

---

### Phase 7: Session-Logging (17.01.2026)

**Neue Datei:** `session_logger.py`

**Features:**
- JSON-Logs pro Session
- Persistenz nach jedem Schritt (auch bei Abbruch)
- Token-Tracking pro Agent
- Kosten-SchÃ¤tzung
- Timeline aller Schritte

**Log-Struktur:**
```json
{
  "session_id": "20260117_215119",
  "timeline": [
    {
      "agent": "Researcher",
      "model": "gpt-4o",
      "duration_ms": 7707,
      "tokens": { "input": 1375, "output": 274 },
      "tool_calls": ["tavily_search"],
      "status": "success"
    }
  ],
  "summary": {
    "total_tokens": { "input": 40271, "output": 17632 },
    "estimated_cost_usd": 0.39
  }
}
```

**Neue API-Endpoints:**
- `GET /api/logs`
- `GET /api/logs/{filename}`
- `GET /api/articles/{filename}/log`

**Frontend:**
- `LogDrawer.tsx` - Details-Button zeigt Session-Log

---

### Phase 8: Intelligente Themenanalyse (17.01.2026)

**Neuer Workflow:** IDLE â†’ **PLANNING** â†’ PRODUCING â†’ COMPLETE

**Features:**
1. **Automatische Themenanalyse:**
   - Orchestrator analysiert Kernfrage
   - Bestimmt: topic_type, time_relevance, complexity
   - SchlÃ¤gt 2-5 Recherche-Runden vor

2. **Plan-Editor (PlanningView.tsx):**
   - Runden aktivieren/deaktivieren
   - Suchanfragen bearbeiten
   - Editor ein-/ausschalten

**Neue Datenstrukturen:**
```python
@dataclass
class ResearchRound:
    name: str
    focus: str
    search_query: str
    enabled: bool = True

@dataclass
class ResearchPlan:
    topic_type: str
    time_relevance: str
    complexity: str
    rounds: List[ResearchRound]
    use_editor: bool
    reasoning: str
```

**Neuer API-Endpoint:**
- `POST /api/analyze` - Themenanalyse

**Bug-Fix: JSON-Parsing (18.01.2026)**

Das LLM gibt manchmal `recommended_rounds` statt `rounds` zurÃ¼ck. Fix in `from_dict`:
```python
# UnterstÃ¼tze sowohl "rounds" als auch "recommended_rounds" (LLM-Variation)
raw_rounds = data.get("rounds") or data.get("recommended_rounds", [])
```

**UI-Entscheidungen:**
- **"Plan erstellen"** â†’ KI analysiert, User kann anpassen
- **"Schnellstart"** â†’ Direkt generieren (KI analysiert intern)
- Alte Slider fÃ¼r Recherche-Runden in Settings entfernt (jetzt dynamisch pro Thema)

---

### Phase 3 Details: React Migration

**Hintergrund:** Streamlit lieferte trotz mehrerer Iterationen kein professionelles UI. React/Tailwind war die bessere Wahl.

**Konzeptionelle Metapher:** "AI Writing Studio" (nicht Dashboard, nicht Editor)

**State-Machine:**
```
IDLE â†’ ANALYZING â†’ PLANNING â†’ PRODUCING â†’ COMPLETE
         â†“            â†“           â†“
      (optional)   (optional)  (streaming)
```

**Server-Setup (zwei Terminals!):**
```bash
# Terminal 1: Backend
cd HayMAS && ./venv/bin/uvicorn api:app --host 0.0.0.0 --port 8000 --reload

# Terminal 2: Frontend
cd HayMAS/frontend && npm run dev
```

**Frontend:** http://localhost:5173  
**Backend API:** http://localhost:8000  
**API Docs:** http://localhost:8000/docs

**Vite Proxy-Config (`vite.config.ts`):**
```typescript
server: {
  proxy: {
    '/api': {
      target: 'http://localhost:8000',
      changeOrigin: true,
    },
  },
}
```

---

## ğŸ”§ Optimierungspotenziale (Detailliert)

**Basierend auf Log-Analyse der Session 20260117_215119:**

```
Gesamtdauer:    4:19 Min (258.839ms)
Total Tokens:   57.903 (40k in / 18k out)
Kosten:         $0.39
Artikel:        3.482 WÃ¶rter
```

---

### ğŸ”´ PRIO 1: Writer Output-LÃ¤nge begrenzen

**Problem:**
```
Writer:     120s (46% der Gesamtzeit)
Revision:    89s (34% der Gesamtzeit)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Summe:      209s von 259s total = 80%!
```

Der Writer produziert sehr lange Texte (25k â†’ 29k Zeichen nach Revision).

**Erwarteter Effekt:** -50% Zeit, -30% Kosten

**Umsetzung in `agents/writer.py`:**
```python
WRITER_SYSTEM_PROMPT = """...
## WICHTIGE EINSCHRÃ„NKUNGEN:
- Maximale ArtikellÃ¤nge: 2500 WÃ¶rter
- Fokussiere auf Kernaussagen, vermeide Redundanz
- Jeder Abschnitt sollte max. 300-400 WÃ¶rter haben
..."""
```

**Oder in `orchestrator.py` beim Writer-Task:**
```python
writer_task = f"""Erstelle einen Wissensartikel zur Kernfrage: {core_question}

WICHTIG: 
- Maximale LÃ¤nge: 2500 WÃ¶rter
- Nutze NUR Informationen aus der Recherche
- Keine Wiederholungen oder FÃ¼lltext
"""
```

**Status:** â³ Nicht umgesetzt

---

### ğŸ”´ PRIO 2: Budget-Modelle Ã¼berdenken

**Problem:**
GPT-5.1 ist als "Budget" konfiguriert, ist aber fast so teuer wie GPT-5.2.

**Aktuelle Konfiguration (`config.py`):**
```python
AGENT_MODELS = {
    "writer": AgentModelConfig(
        premium="gpt-5.2",
        budget="gpt-5.1",  # â† Immer noch teuer!
    ),
}
```

**Preisvergleich (geschÃ¤tzt):**
| Modell | Input/1M | Output/1M | Kategorie |
|--------|----------|-----------|-----------|
| GPT-5.2 | ~$10 | ~$30 | Premium |
| GPT-5.1 | ~$8 | ~$24 | Premium |
| GPT-4o | ~$2.50 | ~$10 | **Echtes Budget** |
| GPT-4o-mini | ~$0.15 | ~$0.60 | **Sehr gÃ¼nstig** |

**Empfohlene Ã„nderung:**
```python
AGENT_MODELS = {
    "orchestrator": AgentModelConfig(
        premium="claude-opus-4-5",
        budget="gpt-4o-mini",  # War: claude-sonnet-4-5
    ),
    "researcher": AgentModelConfig(
        premium="claude-sonnet-4-5",
        budget="gpt-4o-mini",  # War: gpt-4o
    ),
    "writer": AgentModelConfig(
        premium="gpt-5.2",
        budget="gpt-4o",  # War: gpt-5.1
    ),
    "editor": AgentModelConfig(
        premium="claude-sonnet-4-5",
        budget="claude-haiku-4-5",  # OK, Haiku ist gÃ¼nstig
    ),
}
```

**Erwarteter Effekt:** -40% Kosten bei Budget-Runs

**Status:** â³ Nicht umgesetzt

---

### ğŸŸ¡ PRIO 3: Recherche parallelisieren

**Problem:**
```
Research Round 1: 7.7s
Research Round 2: 7.0s
Research Round 3: 8.6s
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Sequenziell:     23.3s
Parallel:        ~8s (lÃ¤ngste Runde)
Ersparnis:       ~15s
```

**Umsetzung:**

Option A: `asyncio` in Python
```python
import asyncio

async def research_parallel(self, rounds: List[ResearchRound]):
    tasks = [
        self._research_single(round) 
        for round in rounds
    ]
    results = await asyncio.gather(*tasks)
    return results
```

Option B: `concurrent.futures`
```python
from concurrent.futures import ThreadPoolExecutor

def research_parallel(self, rounds):
    with ThreadPoolExecutor(max_workers=5) as executor:
        results = list(executor.map(self._research_single, rounds))
    return results
```

**Herausforderung:** 
- SSE-Events mÃ¼ssen trotzdem in richtiger Reihenfolge gesendet werden
- Oder: Events mit Round-ID taggen, Frontend sortiert

**Erwarteter Effekt:** -15s Dauer (bei 3 Runden)

**Status:** â³ Nicht umgesetzt

---

### ğŸŸ¡ PRIO 4: Editor-Prompt anpassen

**Problem:**
```
Artikel VOR Editor:   24.911 Zeichen
Artikel NACH Editor:  29.424 Zeichen (+18%!)
```

Der Editor fordert "mehr Details" statt zu straffen.

**Aktueller Prompt (`orchestrator.py`):**
```python
editor_task = """PrÃ¼fe den folgenden Wissensartikel auf QualitÃ¤t:
1. Ist der Artikel relevant zur Kernfrage?
2. Ist er gut strukturiert?
3. Sind die Informationen korrekt und vollstÃ¤ndig?
4. Gibt es VerbesserungsvorschlÃ¤ge?
"""
```

**Verbesserter Prompt:**
```python
editor_task = """PrÃ¼fe den Wissensartikel kritisch:

## PRÃœFKRITERIEN:
1. Relevanz: Beantwortet der Artikel die Kernfrage?
2. Struktur: Ist die Gliederung logisch?
3. Fakten: Stimmen die Informationen mit der Recherche Ã¼berein?
4. Redundanz: Gibt es Wiederholungen oder FÃ¼lltext?

## FEEDBACK-REGELN:
- Empfehle KÃœRZUNGEN bei redundanten Passagen
- Empfehle KEINE Erweiterungen auÃŸer bei faktischen LÃ¼cken
- Ziel: PrÃ¤gnant und fokussiert, nicht lÃ¤nger
- Wenn der Artikel gut ist: Schreibe "KEINE WEITEREN Ã„NDERUNGEN"
"""
```

**Erwarteter Effekt:** -20% Tokens, kÃ¼rzere Artikel

**Status:** â³ Nicht umgesetzt

---

### ğŸŸ¢ PRIO 5: Fakten-Coverage tracken

**Problem:**
```
Research Input:   ~4.200 Zeichen (3 Runden)
Artikel Output:  ~25.000 Zeichen
VerhÃ¤ltnis:      1:6
```

Der Writer "erfindet" viel Content, der nicht aus der Recherche stammt.

**Idee:** Nach der Generierung prÃ¼fen, welche Research-Fakten verwendet wurden.

**MÃ¶gliche Umsetzung:**
1. **Einfach:** Research-Ergebnisse als "Quellen" nummerieren [1], [2], ...
2. **Mittel:** Writer muss Quellen zitieren, Editor prÃ¼ft Coverage
3. **AufwÃ¤ndig:** Embedding-Vergleich Research â†” Artikel-AbsÃ¤tze

**Beispiel fÃ¼r Writer-Prompt:**
```python
writer_task = """Erstelle einen Wissensartikel.

QUELLENVERWENDUNG:
- Nummeriere jede Recherche-Runde als [Quelle 1], [Quelle 2], etc.
- Zitiere Fakten mit [1], [2], etc. im Text
- Am Ende: Quellenverzeichnis mit allen verwendeten Quellen
"""
```

**Erwarteter Effekt:** +QualitÃ¤t, +Transparenz

**Status:** â³ Nicht umgesetzt

---

### ğŸŸ¢ PRIO 6: Research-Limit erhÃ¶hen

**Problem:**
Tool-Results werden auf 1.500 Zeichen gekÃ¼rzt (MAX_TOOL_RESULT_CHARS).

**Aktuell (`config.py`):**
```python
MAX_TOOL_RESULT_CHARS = 1500  # ~400 Tokens
```

**Trade-off:**
- HÃ¶heres Limit = Mehr Kontext fÃ¼r Writer = Bessere Fakten
- HÃ¶heres Limit = Mehr Tokens = HÃ¶here Kosten

**Empfehlung:**
```python
MAX_TOOL_RESULT_CHARS = 2500  # ~600 Tokens, +66%
```

Bei 3 Runden: +3 Ã— 1000 = 3.000 Zeichen mehr â‰ˆ +750 Tokens â‰ˆ +$0.02

**Status:** â³ Nicht umgesetzt

---

## ğŸ“Š Zusammenfassung Optimierungen

| # | Optimierung | Effekt | Aufwand | Status |
|---|-------------|--------|---------|--------|
| ğŸ”´1 | Writer Output begrenzen | -50% Zeit, -30% Kosten | Gering | â³ |
| ğŸ”´2 | Budget-Modelle (GPT-4o) | -40% Kosten | Gering | â³ |
| ğŸŸ¡3 | Recherche parallelisieren | -15s Dauer | Mittel | â³ |
| ğŸŸ¡4 | Editor-Prompt anpassen | -20% Tokens | Gering | â³ |
| ğŸŸ¢5 | Fakten-Coverage tracken | +QualitÃ¤t | Hoch | â³ |
| ğŸŸ¢6 | Research-Limit erhÃ¶hen | +QualitÃ¤t | Gering | â³ |

**Quick Wins (< 30 Min Aufwand):**
- Budget-Modelle Ã¤ndern (config.py)
- Writer-Prompt anpassen (orchestrator.py)
- Editor-Prompt anpassen (orchestrator.py)
- Research-Limit erhÃ¶hen (config.py)

---

## ğŸ“ Wichtige Dateien

### Backend
- `api.py` - FastAPI Server mit allen Endpoints
- `config.py` - Alle Konfigurationen (API-Keys, Modelle, Limits)
- `session_logger.py` - JSON-Logging mit Kosten-Tracking
- `agents/orchestrator.py` - Hauptlogik + `analyze_topic()` + `ResearchPlan`
- `agents/base_agent.py` - ReAct-Loop fÃ¼r alle Agents
- `agents/researcher.py` - Tavily Web-Suche
- `agents/writer.py` - Artikel-Generierung
- `agents/editor.py` - QualitÃ¤tsprÃ¼fung

**Alle API-Endpoints:**
| Endpoint | Methode | Beschreibung |
|----------|---------|--------------|
| `/` | GET | API-Info + Links |
| `/api/status` | GET | API-Key Status |
| `/api/models` | GET | VerfÃ¼gbare Modelle |
| `/api/articles` | GET | Liste aller Artikel |
| `/api/articles/{filename}` | GET | Artikel-Inhalt |
| `/api/articles/{filename}/log` | GET | Session-Log zum Artikel |
| `/api/logs` | GET | Alle Logs |
| `/api/logs/{filename}` | GET | Einzelnes Log |
| `/api/analyze` | POST | Themenanalyse â†’ Plan |
| `/api/generate` | POST | Artikel generieren (SSE) |

### Frontend
- `frontend/src/components/Studio.tsx` - Haupt-Container, routet zwischen Views
- `frontend/src/components/IdleView.tsx` - Eingabe + "Plan erstellen" / "Schnellstart"
- `frontend/src/components/PlanningView.tsx` - Plan-Editor mit Toggle/Edit
- `frontend/src/components/ProducingView.tsx` - Live Event-Stream wÃ¤hrend Generierung
- `frontend/src/components/CompleteView.tsx` - Fertiger Artikel mit Download
- `frontend/src/components/Header.tsx` - Top-Navigation mit API-Status
- `frontend/src/components/ArchiveDrawer.tsx` - Liste aller Artikel
- `frontend/src/components/SettingsDrawer.tsx` - Modell-Tiers (Premium/Budget)
- `frontend/src/components/LogDrawer.tsx` - Session-Log Details
- `frontend/src/hooks/useStudio.ts` - Zentrale State-Machine
- `frontend/src/lib/api.ts` - API-Client (fetch + SSE)
- `frontend/src/types/index.ts` - TypeScript Interfaces

### Output
- `output/` - Generierte Markdown-Artikel
- `logs/` - Session-Logs (JSON)

---

## ğŸ¯ Dynamische Themenanalyse - Beispiele

Die KI passt die Recherche-Strategie automatisch an:

| Frage | topic_type | Runden | Editor |
|-------|------------|--------|--------|
| "Warum ist die Banane krumm?" | science | 2 | Nein |
| "Was sind aktuelle KI Tools fÃ¼r AI Coding?" | tech | 4 | Nein |
| "Rolle der Frau im Nazi-Deutschland" | history | 4-5 | Ja |
| "Wie funktioniert RAG?" | tech | 3 | Nein |

**Analysierte Dimensionen:**
- `topic_type`: tech, science, history, business, culture, general
- `time_relevance`: current, recent, historical, timeless
- `complexity`: simple, medium, complex
- `needs_current_data`: true/false
- `geographic_focus`: global, regional, local, none

---

## ğŸš€ NÃ¤chste Schritte (Ideen)

1. **Parallelisierung der Recherche** - ~15s Zeitersparnis
2. **Fakten-Coverage Tracking** - PrÃ¼fen ob Research genutzt wird
3. **Quellen-Annotation** - [1], [2], ... im Artikel
4. **Budget-Modelle anpassen** - GPT-4o statt GPT-5.1
5. **Writer Output begrenzen** - Max. 2500 WÃ¶rter
6. **Drag & Drop fÃ¼r Recherche-Runden** - Reihenfolge Ã¤ndern

---

## ğŸ“ Kontakt

Entwickelt mit Cursor AI.
Stand: 18. Januar 2026
